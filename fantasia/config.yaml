# ==============================================================================
# FANTASIA — Configuration
# Technical, production-oriented YAML with explicit separation of concerns.
# ==============================================================================

# ------------------------------------------------------------------------------
# Global execution
# ------------------------------------------------------------------------------
log_path: ~/fantasia/logs/    # str  | Directory for pipeline logs (will be created if missing)
constants: ./fantasia/constants.yaml # str  | Directory for constants
base_directory: ~/fantasia/   # str  | Root working dir: inputs, outputs, caches, temp live here
prefix: sample # str | Output prefix (folder & file stem)

limit_execution: 0
monitor_interval: 5

input: data_sample/sample.fasta  # str | Default FASTA (can be overridden via CLI --input)
only_lookup: True            # bool | If true: skip Stage A (embedding) and run Stage B using existing embeddings.h5

# ------------------------------------------------------------------------------
# Database (PIS: PostgreSQL + pgvector)
# Used by Stage B to load in-memory reference tables (IDs, vectors, GO)
# ------------------------------------------------------------------------------
DB_USERNAME: usuario          # str  | PostgreSQL username
DB_PASSWORD: clave            # str  | PostgreSQL password
DB_HOST: localhost            # str  | PostgreSQL host (use a socket path if local with custom setup)
DB_PORT: 5432                 # int  | PostgreSQL port
DB_NAME: BioData              # str  | PostgreSQL database name

# ------------------------------------------------------------------------------
# Message broker (RabbitMQ)
# Used by Stage A to publish/consume embedding tasks
# ------------------------------------------------------------------------------
rabbitmq_host: localhost      # str  | Broker host
rabbitmq_port: 5672           # int  | Broker port
rabbitmq_user: guest          # str  | Broker user
rabbitmq_password: guest      # str  | Broker password

delete_queues: true           # bool | Drop queues on boot/teardown to avoid stale messages between runs

# ------------------------------------------------------------------------------
# External data source (optional)
# URL used by `fantasia initialize` to preload a DB dump or lookup tables
# ------------------------------------------------------------------------------
embeddings_url: "https://zenodo.org/.../lookup_table_exp_v3.tar?download=1"  # str | Leave empty to skip download

# ==============================================================================
# Stage A — Embedding
# Produces HDF5 with per-layer query embeddings (no DB required here).
# ==============================================================================
embedding:
  device: cuda                # enum{cuda,cpu} | Primary device for PLMs; set to cpu to force CPU execution
  queue_batch_size: 100       # int  | Number of sequences per published batch to RabbitMQ
  max_sequence_length: 0      # int  | 0 disables truncation; otherwise sequences are truncated to this length

  # Per-model configuration.
  # name: logical model identifier used across the pipeline (case-sensitive).
  # enabled: if false, model is ignored at runtime.
  # batch_size: PLM forward batch size (beware of VRAM limits).
  # layer_index: list[int] of layers to export; use multiple to materialize several layers into HDF5.
  models:
    ESM:
      enabled: true
      batch_size: 1
      layer_index: [16, 24, 28, 33]
      distance_threshold: 0

    ESM3c:
      enabled: true
      batch_size: 1
      layer_index: [ 18,24,30,35 ]
      distance_threshold: 0

    Ankh3-Large:
      enabled: true
      batch_size: 1
      layer_index: [ 24,32,40,48 ]
      distance_threshold: 0

    Prot-T5:
      enabled: true
      batch_size: 1
      layer_index: [ 12,16,20,24 ]
      distance_threshold: 0

    Prost-T5:
      enabled: true
      batch_size: 1
      layer_index: [ 12,16,20,24 ]
      distance_threshold: 0

# HDF5 on-disk layout (for reference; produced by Stage A):
# /accession_<id>/type_<embedding_type_id>/layer_<k>/embedding
# (sequence is stored at the accession level)

# ==============================================================================
# Stage B — Lookup
# Consumes embeddings.h5 + in-memory references (IDs, vectors, GO) to produce CSV/TSV.
# ==============================================================================
lookup:
  use_gpu: True               # bool | If true, run vector distances on GPU when available
  batch_size: 516            # int  | Vector distance batch size (tune to GPU memory)
  distance_metric: cosine     # enum{cosine,euclidean} | Distance for nearest-neighbor search
  limit_per_entry: 5          # int  | k neighbors returned per query (a.k.a. “k”)
  lookup_cache_max: 4       # int  | Max entries per (model,layer) in-memory cache (tune to RAM)
  topgo: true                 # bool | If true, emit TopGO-compatible TSV alongside CSV outputs


  # Redundancy filtering (optional pre-filter on reference side, e.g., MMseqs2).
  redundancy:
    identity: 0.0             # float in [0,1] | 0 disables; 1.0 = 100% identity (strict deduplication)
    coverage: 0.7             # float in (0,1]  | Alignment coverage threshold used in deduplication
    threads: 10               # int  | CPU threads for redundancy filtering tools

  # Taxonomy filters (applied after NN retrieval to prune/keep specific taxa).
  taxonomy:
    exclude: [ "559292" ]              # list[str] | Taxonomy IDs to exclude (e.g., ["559292","6239"])
    include_only: [ ]          # list[str] | If non-empty, restrict results to these IDs (takes precedence)
    get_descendants: false    # bool | If true, expand filters to include descendants


# ------------------------------------------------------------------------------
# Post-processing
# Scoring/collapsing to GO leaf terms, reliability index derivation, model/layer consolidation.
# ------------------------------------------------------------------------------
postprocess:
  keep_sequences: true
  summary:
    include_counts: true
    normalize_count_by_limit_per_entry: true   # por defecto True si lo omites abajo
    export_raw_count: true                    # ponlo a true si quieres "count_raw"
    metrics:
      reliability_index: [ min, max, mean ]
      identity: [ min, max, mean ]
      identity_sw: [ min, max, mean ]
    aliases:
      reliability_index: ri
      identity: id_g
      identity_sw: id_l

    weights:
      # 1) Por métrica y función:
      reliability_index: { max: 0.6 }
      # 2) Por métrica/alias con un único número (aplica a min/max/mean):
      max_id_g: 0.2
      # 3) Por nombre de salida (agg + alias):
      max_id_l: 0.3
      # Opcional: también puedes ponderar el count
      count: 0.3
    # Prefijo para las columnas ponderadas (opcional; por defecto "w_")
    weighted_prefix: "w_"


# ------------------------------------------------------------------------------
# Output structure (produced by Stage B; for reference)
# ------------------------------------------------------------------------------
# raw_results_layer_{k}.csv  # per-layer raw neighbors with distances
# raw_results.csv            # legacy raw (no explicit layer)
# results_scored.csv         # scored results after RI + weighting
# results_collapsed.csv      # collapsed across models/layers (best evidence kept)
# results.csv                # final alias to collapsed (for downstream)
# results_topgo.tsv          # optional TopGO table (if lookup.topgo = true)
