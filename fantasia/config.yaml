# ==============================================================================
# FANTASIA — Configuration (refactored)
# Technical, production-oriented YAML with explicit separation of concerns.
# ==============================================================================

# ------------------------------------------------------------------------------
# Global execution
# ------------------------------------------------------------------------------
log_path: ~/fantasia/logs/    # str  | Directory for pipeline logs (will be created if missing)
constants: ./fantasia/constants.yaml # str  | Directory for constants
base_directory: ~/fantasia/   # str  | Root working dir: inputs, outputs, caches, temp live here
prefix: cafa5_multiple_layers_k_5 # str | Output prefix (folder & file stem)

input: data_sample/sample.fasta  # str | Default FASTA (can be overridden via CLI --input)
only_lookup: false            # bool | If true: skip Stage A (embedding) and run Stage B using existing embeddings.h5

# ------------------------------------------------------------------------------
# Database (PIS: PostgreSQL + pgvector)
# Used by Stage B to load in-memory reference tables (IDs, vectors, GO)
# ------------------------------------------------------------------------------
DB_USERNAME: usuario          # str  | PostgreSQL username
DB_PASSWORD: clave            # str  | PostgreSQL password
DB_HOST: localhost            # str  | PostgreSQL host (use a socket path if local with custom setup)
DB_PORT: 5432                 # int  | PostgreSQL port
DB_NAME: BioData              # str  | PostgreSQL database name

# ------------------------------------------------------------------------------
# Message broker (RabbitMQ)
# Used by Stage A to publish/consume embedding tasks
# ------------------------------------------------------------------------------
rabbitmq_host: localhost      # str  | Broker host
rabbitmq_port: 5672           # int  | Broker port
rabbitmq_user: guest          # str  | Broker user
rabbitmq_password: guest      # str  | Broker password

delete_queues: true           # bool | Drop queues on boot/teardown to avoid stale messages between runs

# ------------------------------------------------------------------------------
# External data source (optional)
# URL used by `fantasia initialize` to preload a DB dump or lookup tables
# ------------------------------------------------------------------------------
embeddings_url: "https://zenodo.org/.../lookup_table_exp_v3.tar?download=1"  # str | Leave empty to skip download

# ==============================================================================
# Stage A — Embedding
# Produces HDF5 with per-layer query embeddings (no DB required here).
# ==============================================================================
embedding:
  device: cuda                # enum{cuda,cpu} | Primary device for PLMs; set to cpu to force CPU execution
  queue_batch_size: 100       # int  | Number of sequences per published batch to RabbitMQ
  max_sequence_length: 0      # int  | 0 disables truncation; otherwise sequences are truncated to this length

  # Per-model configuration.
  # name: logical model identifier used across the pipeline (case-sensitive).
  # enabled: if false, model is ignored at runtime.
  # batch_size: PLM forward batch size (beware of VRAM limits).
  # layer_index: list[int] of layers to export; use multiple to materialize several layers into HDF5.
  models:
    ESM:
      enabled: true
      batch_size: 1
      layer_index: [33]       # Example layer; adjust to your checkpoint
    ESM3c:
      enabled: true
      batch_size: 1
      layer_index: [35]
    Ankh3-Large:
      enabled: true
      batch_size: 1
      layer_index: [48]
    Prot-T5:
      enabled: true
      batch_size: 1
      layer_index: [24]
    Prost-T5:
      enabled: true
      batch_size: 1
      layer_index: [24]

# HDF5 on-disk layout (for reference; produced by Stage A):
# /accession_<id>/type_<embedding_type_id>/layer_<k>/embedding
# (sequence is stored at the accession level)

# ==============================================================================
# Stage B — Lookup
# Consumes embeddings.h5 + in-memory references (IDs, vectors, GO) to produce CSV/TSV.
# ==============================================================================
lookup:
  use_gpu: false               # bool | If true, run vector distances on GPU when available
  batch_size: 516            # int  | Vector distance batch size (tune to GPU memory)
  distance_metric: euclidean     # enum{cosine,euclidean} | Distance for nearest-neighbor search
  limit_per_entry: 5          # int  | k neighbors returned per query (a.k.a. “k”)
  lookup_cache_max: 4       # int  | Max entries per (model,layer) in-memory cache (tune to RAM)
  topgo: true                 # bool | If true, emit TopGO-compatible TSV alongside CSV outputs


  # Redundancy filtering (optional pre-filter on reference side, e.g., MMseqs2).
  redundancy:
    identity: 0.0             # float in [0,1] | 0 disables; 1.0 = 100% identity (strict deduplication)
    coverage: 0.7             # float in (0,1]  | Alignment coverage threshold used in deduplication
    threads: 0               # int  | CPU threads for redundancy filtering tools

  # Taxonomy filters (applied after NN retrieval to prune/keep specific taxa).
  taxonomy:
    exclude: ["559292","6239"]              # list[str] | Taxonomy IDs to exclude (e.g., ["559292","6239"])
    include_only: []          # list[str] | If non-empty, restrict results to these IDs (takes precedence)
    get_descendants: false    # bool | If true, expand filters to include descendants


# ------------------------------------------------------------------------------
# Post-processing
# Scoring/collapsing to GO leaf terms, reliability index derivation, model/layer consolidation.
# ------------------------------------------------------------------------------
postprocess:
  keep_sequences: true        # bool | Keep input and reference sequences in final outputs when available
  # Weighting knobs for final score; keep 0 to disable a term’s contribution.
  weights:
    reliability_index: 0.5    # float | Derived from distance (lower distance → higher RI)
    support_count_norm: 0.0   # float | Count-based support normalization
    collapsed_support_norm: 0.0
    model_consistency: 0.25   # float | Agreement across models
    alignment_norm: 0.25      # float | Alignment-based normalization (when available)
    layer_support_norm: 0.0   # float | Agreement across layers (when multiple exported)

# ------------------------------------------------------------------------------
# Output structure (produced by Stage B; for reference)
# ------------------------------------------------------------------------------
# raw_results_layer_{k}.csv  # per-layer raw neighbors with distances
# raw_results.csv            # legacy raw (no explicit layer)
# results_scored.csv         # scored results after RI + weighting
# results_collapsed.csv      # collapsed across models/layers (best evidence kept)
# results.csv                # final alias to collapsed (for downstream)
# results_topgo.tsv          # optional TopGO table (if lookup.topgo = true)
