# ==============================================================================
# FANTASIA — Configuration
# Technical, production-oriented YAML with explicit separation of concerns.
# ==============================================================================

# ------------------------------------------------------------------------------
# Global execution
# ------------------------------------------------------------------------------
log_path: ~/fantasia/logs/    # str  | Directory for pipeline logs (will be created if missing)
constants: ./fantasia/constants.yaml # str  | Directory for constants
base_directory: ~/fantasia/   # str  | Root working dir: inputs, outputs, caches, temp live here
prefix: merged
 # str | Output prefix (folder & file stem)

limit_execution: 0
monitor_interval: 10

input: data_sample/testsuperset_cafa6.fasta  # str | Default FASTA (can be overridden via CLI --input)
only_lookup: False            # bool | If true: skip Stage A (embedding) and run Stage B using existing embeddings.h5

# ------------------------------------------------------------------------------
# Database (PIS: PostgreSQL + pgvector)
# Used by Stage B to load in-memory reference tables (IDs, vectors, GO)
# ------------------------------------------------------------------------------
DB_USERNAME: usuario          # str  | PostgreSQL username
DB_PASSWORD: clave            # str  | PostgreSQL password
DB_HOST: localhost            # str  | PostgreSQL host (use a socket path if local with custom setup)
DB_PORT: 5432                 # int  | PostgreSQL port
DB_NAME: BioData              # str  | PostgreSQL database name

# ------------------------------------------------------------------------------
# Message broker (RabbitMQ)
# Used by Stage A to publish/consume embedding tasks
# ------------------------------------------------------------------------------
rabbitmq_host: localhost      # str  | Broker host
rabbitmq_port: 5672           # int  | Broker port
rabbitmq_user: guest          # str  | Broker user
rabbitmq_password: guest      # str  | Broker password

delete_queues: true           # bool | Drop queues on boot/teardown to avoid stale messages between runs

# ------------------------------------------------------------------------------
# External data source (optional)
# URL used by `fantasia initialize` to preload a DB dump or lookup tables
#
# Available options:
# 1. Final layer only (smaller, faster to load) [default]
#    https://zenodo.org/records/17167843/files/FANTASIA_UniProt_Sep2025_Last_ExpOnly.dump?download=1
#
# 2. Final + intermediate layers (larger, more detailed embeddings)
#    https://zenodo.org/records/17151847/files/FANTASIA_UniProt_Sep2025_Final+Interm_ExpOnly.dump?download=1
#
# Leave empty to skip the preload step.
# ------------------------------------------------------------------------------
embeddings_url: "https://zenodo.org/records/17167843/files/FANTASIA_UniProt_Sep2025_Last_ExpOnly.dump?download=1"
# embeddings_url: "https://zenodo.org/records/17151847/files/FANTASIA_UniProt_Sep2025_Final+Interm_ExpOnly.dump?download=1"

# ==============================================================================
# Stage A — Embedding
# Produces HDF5 with per-layer query embeddings (no DB required here).
# ==============================================================================
embedding:
  device: cuda                # enum{cuda,cpu} | Primary device for PLMs; set to cpu to force CPU execution
  queue_batch_size: 1000       # int  | Number of sequences per published batch to RabbitMQ
  max_sequence_length: 1516      # int  | 0 disables truncation; otherwise sequences are truncated to this length

  # Per-model configuration.
  # name: logical model identifier used across the pipeline (case-sensitive).
  # enabled: if false, model is ignored at runtime.
  # batch_size: PLM forward batch size (beware of VRAM limits).
  # layer_index: list[int] of layers to export; use multiple to materialize several layers into HDF5.
  # LAYER INDEXING NOTE: 0 = last (output) layer, 1 = penultimate, 2 = second-to-last, and so on.

  # If you use multiple layers, make sure the reference set database has been initialized with those layers.

  models:
    ESM: # 34 layers: 0..33
      enabled: False
      batch_size: 1
      layer_index: [ 0,1,2,16,17,18 ]

    ESM3c: # 36 layers: 0..35
      enabled: True
      batch_size: 1
      layer_index: [ 0,2 ]
      distance_threshold: 0

    Ankh3-Large: # 49 layers: 0..48
      enabled: False
      batch_size: 1
      layer_index: [ 0,1,2,23,24,25 ]
      distance_threshold: 0

    Prot-T5: # 25 layers: 0..24
      enabled: False
      batch_size: 1
      layer_index: [ 0, 1, 2, 11, 12, 13 ]
      distance_threshold: 0

    Prost-T5: # 25 layers: 0..24 (same backbone as Prot-T5)
      enabled: False
      batch_size: 1
      layer_index: [ 0, 1, 2, 11, 12, 13 ]
      distance_threshold: 0


# HDF5 on-disk layout (for reference; produced by Stage A):
# /accession_<id>/type_<embedding_type_id>/layer_<k>/embedding
# (sequence is stored at the accession level)

# ==============================================================================
# Stage B — Lookup
# Consumes embeddings.h5 + in-memory references (IDs, vectors, GO) to produce CSV/TSV.
# ==============================================================================
lookup:

  compute:
    use_gpu: true
    batch_size: 5
    distance_metric: cosine

  nn:
    limit_per_entry: 10
    cache_max: 1

  filters:
    redundancy:
      identity: 0
      coverage: 0.7
      threads: 10

    taxonomy:
      exclude: []
      include_only: []
      get_descendants: false

  selection:
    enabled: true
    thresholds:
      strong:
        min_identity: 0.40
        max_identity: 1.0
      intermediate:
        min_identity: 0.25
        max_identity: 0.40
      weak:
        min_identity: 0.00
        max_identity: 0.25

    categories:
      C: { K_good: 3, intermediate_multiplier: 2, weak_multiplier: 3 }
      F: { K_good: 3, intermediate_multiplier: 2, weak_multiplier: 3 }
      P: { K_good: 3, intermediate_multiplier: 2, weak_multiplier: 3 }

  output:
    topgo: true
    precision: 4



# ------------------------------------------------------------------------------
# Post-processing
# Scoring, collapsing, reliability index derivation, model/layer consolidation.
# ------------------------------------------------------------------------------
postprocess:
  keep_sequences: False
  summary:
    include_counts: true
    normalize_count_by_limit_per_entry: true   # defaults to True if omitted below
    export_raw_count: true                     # set to true if you want "count_raw"
    metrics:
      reliability_index: [ max ]
      identity: [ min, max, mean ]
      identity_sw: [ min, max, mean ]
    aliases:
      reliability_index: ri
      identity: id_g
      identity_sw: id_l

    weights:
      # 1) By metric and aggregation function:
      reliability_index: { max: 0.4 }
      # 2) By metric/alias with a single number (applies to min/max/mean):
      max_id_g: 0.20
      # 3) By full output name (agg + alias):
      max_id_l: 0.20
      # Optional: you can also weight the count
      count: 0.2

    # Prefix for weighted columns (optional; defaults to "w_")
    weighted_prefix: "w_"



# ------------------------------------------------------------------------------
# Output structure (Stage B — generated by the current code)
# ------------------------------------------------------------------------------
# Per-query/model/layer shards:
#   <experiment_path>/raw_results/{model_name}/layer_{k|legacy}/{accession}.csv
# Global summary (post_processing):
#   <experiment_path>/summary.csv
# TopGO (if lookup.topgo = true):
#   <experiment_path>/topgo/{model_name}/layer_{k|legacy}/{BP|MF|CC}.topgo
#   <experiment_path>/topgo/ensemble/{BP|MF|CC}.topgo
# Global FASTA of all sequences:
#   <experiment_path>/{sequences_fasta}  # IDs: Q{idx}/R{idx}
#